# For example:
multiprocessing: False

path_pretrained_models: './pretrained_models'
dataset:
    data_path: 'data/V_Star'
    dataset_name: 'V_Star'
    task_type : 'direct_attributes'
    dataset_size: 'full'
    # captions: True
blip_v2_model_type: blip2-flan-t5-xxl  # Change to blip2-flan-t5-xl for smaller GPUs
blip_half_precision: True
# Add more changes here, following the same format as base_config.yaml
execute_code: True  

gpt3:                                               # GPT-3 configuration
    n_votes: 1                                      # Number of tries to use for GPT-3. Use with temperature > 0
    qa_prompt: ./prompts/gpt3/gpt3_qa.txt
    guess_prompt: ./prompts/gpt3/gpt3_process_guess.txt
    temperature: 0.                                 # Temperature for GPT-3. Almost deterministic if 0
    model: chatgpt 

codex:
    temperature: 0.2                                 # Temperature for Codex. (Almost) deterministic if 0
    best_of: 1                                      # Number of tries to choose from. Use when temperature > 0
    max_tokens: 512                                 # Maximum number of tokens to generate for Codex
    # prompt: ./prompts/benchmarks/v_star.prompt                # Codex prompt file, which defines the API. (doesn't support video for now due to token limits)
    # prompt: ./prompts/benchmarks/v_star_visual_search.prompt
    prompt: ./prompts/benchmarks/v_star_visual_search.prompt
    model: gpt-3.5-turbo 

select_answer_prompt: ./prompts/gpt3/v_star_options.txt

# load_models: 
#     codex: False
#     codellama: True
# codex:
#     model: codellama  
#     codellama_model_name: /data/datasets/models/huggingface/meta-llama/CodeLlama-34b-Python-hf #/path/to/code_llama_models/CodeLlama-34b-Python-hf

